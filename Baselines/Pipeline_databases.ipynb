{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-SSHIBA\n",
    "\n",
    "In this notebook we want to analyse the response of the kernel version of the previously presented SSHIBA algorithm with different datasets and baselines.\n",
    "\n",
    "## Loading datasets\n",
    "\n",
    "First of all we will load the specified dataset to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded database: satellite\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import scipy.io as sio\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "foldername = os.path.basename(dirpath)\n",
    "(prv_fold,foldername) = os.path.split(dirpath)\n",
    "os.sys.path.append(prv_fold +'/lib/')\n",
    "os.sys.path.append(prv_fold +'\\\\lib\\\\')\n",
    "import sshiba\n",
    "import sshiba_areas\n",
    "\n",
    "database = 'Satellite' #Here we specify the desired database\n",
    "print('Loaded database: '+database)\n",
    "\n",
    "file = 'data_'+database\n",
    "\n",
    "(prv_2_fold,foldername) = os.path.split(prv_fold)\n",
    "X = np.loadtxt(prv_2_fold+'/Databases/'+file+'/data.txt')\n",
    "Y = np.loadtxt(prv_2_fold+'/Databases/'+file+'/labels.txt')[:,np.newaxis]\n",
    "\n",
    "# =================================================== #\n",
    "# Don't run, just to generate folds and save in a file\n",
    "# =================================================== #\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# skf_tst = StratifiedKFold(n_splits=10, shuffle = True)\n",
    "# fold_tst =[f for  i, f in enumerate(skf_tst.split(X, Y))]\n",
    "# dict_fold_val = {}\n",
    "# for ii, f_tst in enumerate(fold_tst):\n",
    "#     pos_tr = f_tst[0]\n",
    "#     skf_val = StratifiedKFold(n_splits=10, shuffle = True)\n",
    "#     fold_val =[f for  i, f in enumerate(skf_val.split(X[pos_tr], Y[pos_tr]))]\n",
    "#     dict_fold_val[ii]=fold_val\n",
    "\n",
    "# pickle.dump([fold_tst, dict_fold_val], open( 'folds_'+database+'.p', \"wb\" ))\n",
    "\n",
    "# =================================================== #\n",
    "\n",
    "[fold_tst, dict_fold_val] = pickle.load(open('folds_'+database+'.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database is loaded and the partitions are defined we can start to analyse the performance of the algorithm on different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-PCA\n",
    "\n",
    "In this section we will calculate the performance of the kernel version of PCA on this database. In particular, we will not validate the parameters associated to this algorithm ($\\gamma$ and $K_c$), they will be statistically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_sig(X1, X2, sig=0):\n",
    "    size1 = X1.shape[0];\n",
    "    size2 = X2.shape[0];\n",
    "    if X1.ndim==1:\n",
    "        X1 = X1[:,np.newaxis]\n",
    "        X2 = X2[:,np.newaxis]\n",
    "    G = (X1* X1).sum(axis=1)\n",
    "    H = (X2* X2).sum(axis=1)\n",
    "    Q = np.tile(G, [size2,1]).T\n",
    "    R = np.tile(H, [size1,1])\n",
    "    KK=np.dot(X1,X2.T)\n",
    "    dist=(Q + R - 2*KK)\n",
    "    if sig == 0:  # Then, we estimate its value\n",
    "        aux = dist-np.tril(dist)\n",
    "        aux = aux.reshape(size1**2,1)\n",
    "        sig = np.sqrt(0.5*np.mean(aux[np.where(aux>0)]))             \n",
    "    K = np.exp(-dist/sig**2);\n",
    "    return K, sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------> Fold 0 <---------\n",
      "---------> Fold 1 <---------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c88a95dffc59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_tst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_tst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mK_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbf_kernel_sig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mK_tst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbf_kernel_sig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-b651b4689315>\u001b[0m in \u001b[0;36mrbf_kernel_sig\u001b[1;34m(X1, X2, sig)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msize2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msize1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mKK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mKK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Then, we estimate its value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "filename = 'Results/Baselines_'+database+'.pkl'\n",
    "if os.path.exists(filename):\n",
    "    print (\"Loading existing model...\")\n",
    "    my_dict = pickle.load( open( filename, \"rb\" ) )\n",
    "    if 'KPCA' in my_dict['models']:\n",
    "        results = my_dict['models']\n",
    "        print (\"... Model loaded\")\n",
    "    else:\n",
    "        results['KPCA'] = np.zeros((len(fold_tst),))\n",
    "else:\n",
    "    results = {}\n",
    "    results['KPCA'] = np.zeros((len(fold_tst),))\n",
    "\n",
    "for i in np.arange(len(fold_tst)):\n",
    "    \n",
    "    print('---------> Fold '+str(i)+' <---------')   \n",
    "    \n",
    "    if results['KPCA'][i] ! = 0:\n",
    "        # Splitting the data into training and test sets.\n",
    "        pos_tr = fold_tst[i][0]\n",
    "        pos_tst =  fold_tst[i][1]\n",
    "        Y_tr = Y[pos_tr,:] \n",
    "        Y_tst = Y[pos_tst,:]\n",
    "        X_tr = X[pos_tr,:]    \n",
    "        X_tst = X[pos_tst,:]\n",
    "\n",
    "        # Generating RBF kernel and calculating the gamma value.\n",
    "        K_tr, sig = rbf_kernel_sig(X_tr, X_tr)\n",
    "        K_tst, sig = rbf_kernel_sig(X_tst, X_tr, sig = sig)\n",
    "\n",
    "        # Defining the feature extracting algorithm, PCA.\n",
    "        pca = PCA()\n",
    "        P_tr = pca.fit_transform(K_tr)\n",
    "        P_tst = pca.fit_transform(K_tst)\n",
    "\n",
    "        # Selecting the latent factors that explain 95% of the variance.\n",
    "        Kc = 0\n",
    "        while np.sum(pca.explained_variance_ratio_[:Kc]) < 0.95:\n",
    "            Kc = Kc + 1 \n",
    "        Kc_PCA[fold] = Kc\n",
    "        P_tr = P_tr[:,:Kc]\n",
    "        P_tst = P_tst[:,:Kc]\n",
    "\n",
    "        # Training the linear classifier. Hyperparamiters determined using grid search cross validation.\n",
    "        grid = {\"C\": np.logspace(-5,5,11)}# l1 lasso l2 ridge\n",
    "        clf = SVC(kernel = 'linear')\n",
    "        clf_cv = GridSearchCV(clf, grid, cv=10)\n",
    "        clf_cv.fit(P_tr,Y_tr)\n",
    "        results['KPCA'][i] = clf_cv.score(P_tst,Y_tst)   \n",
    "        print('KPCA accuracy: %0.2f%%' %(ACC_KPCA[i]*100))\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        print('Fold previously trained. KPCA accuracy: %0.2f%%' %(results['KPCA'][i]*100))\n",
    "print('KPCA mean accuracy: %0.2f +/- %0.2f%%' %(np.mean(results['KPCA']*100) , np.std(results['KPCA']*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pca = PCA()\n",
    "P_tr = pca.fit_transform(K_tr)\n",
    "P_tst = pca.fit_transform(K_tst)\n",
    "\n",
    "Kc = 0\n",
    "while np.sum(pca.explained_variance_ratio_[:Kc]) < 0.95:\n",
    "    Kc = Kc + 1 \n",
    "Kc_PCA[fold] = Kc\n",
    "P_tr = P_tr[:,:Kc]\n",
    "P_tst = P_tst[:,:Kc]\n",
    "\n",
    "# Grid search cross validation\n",
    "grid = {\"C\": np.logspace(-5,5,11)}# l1 lasso l2 ridge\n",
    "clf = SVC(kernel = 'linear')\n",
    "clf_cv = GridSearchCV(clf, grid, cv=10)\n",
    "clf_cv.fit(P_tr,Y_tr)\n",
    "ACC[i] = clf_cv.score(P_tst,Y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
