{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-SSHIBA\n",
    "\n",
    "In this notebook we want to analyse the response of the kernel version of the previously presented SSHIBA algorithm with different datasets and baselines.\n",
    "\n",
    "## Loading datasets\n",
    "\n",
    "First of all we will load the specified dataset to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded database: Satellite\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2folds_Satellite.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0d6ced5397e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# =================================================== #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mfold_tst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_fold_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'folds_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2folds_Satellite.p'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "foldername = os.path.basename(dirpath)\n",
    "(prv_fold,foldername) = os.path.split(dirpath)\n",
    "os.sys.path.append(prv_fold +'/lib/')\n",
    "os.sys.path.append(prv_fold +'\\\\lib\\\\')\n",
    "\n",
    "database = 'Satellite' #Here we specify the desired database\n",
    "print('Loaded database: '+database)\n",
    "\n",
    "file = 'data_'+database\n",
    "\n",
    "X = np.loadtxt(prv_fold+'/Databases/'+file+'/data.txt')\n",
    "Y = np.loadtxt(prv_fold+'/Databases/'+file+'/labels.txt')\n",
    "\n",
    "folds = 2\n",
    "# =================================================== #\n",
    "# Don't run, just to generate folds and save in a file\n",
    "# =================================================== #\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf_tst = StratifiedKFold(n_splits=folds, shuffle = True)\n",
    "# fold_tst =[f for  i, f in enumerate(skf_tst.split(X, Y))]\n",
    "# dict_fold_val = {}\n",
    "# for ii, f_tst in enumerate(fold_tst):\n",
    "#     pos_tr = f_tst[0]\n",
    "#     skf_val = StratifiedKFold(n_splits=folds, shuffle = True)\n",
    "#     fold_val =[f for  i, f in enumerate(skf_val.split(X[pos_tr], Y[pos_tr]))]\n",
    "#     dict_fold_val[ii]=fold_val\n",
    "\n",
    "# pickle.dump([fold_tst, dict_fold_val], open( str(folds)+'folds_'+database+'.p', \"wb\" ))\n",
    "\n",
    "# =================================================== #\n",
    "\n",
    "[fold_tst, dict_fold_val] = pickle.load(open(str(folds)+'folds_'+database+'.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database is loaded and the partitions are defined we can start to analyse the performance of the algorithm on different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "In this section we will calculate the performance of the kernel version of PCA as well as the kernel version of CCA and GPLVMs on this database. In particular, we will not validate the parameters associated to this algorithm ($\\gamma$ and $K_c$), they will be statistically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAUC(Y_pred, Y_tst):\n",
    "    n_classes = Y_pred.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = np.zeros((n_classes,1))\n",
    "    for i in np.arange(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y_tst[:,i], Y_pred[:,i]/n_classes)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    return roc_auc.flatten()\n",
    "\n",
    "def rbf_kernel_sig(X1, X2, sig=0):\n",
    "    size1 = X1.shape[0];\n",
    "    size2 = X2.shape[0];\n",
    "    if X1.ndim==1:\n",
    "        X1 = X1[:,np.newaxis]\n",
    "        X2 = X2[:,np.newaxis]\n",
    "    G = (X1* X1).sum(axis=1)\n",
    "    H = (X2* X2).sum(axis=1)\n",
    "    Q = np.tile(G, [size2,1]).T\n",
    "    R = np.tile(H, [size1,1])\n",
    "    KK=np.dot(X1,X2.T)\n",
    "    dist=(Q + R - 2*KK)\n",
    "    if sig == 0:  # Then, we estimate its value\n",
    "        aux = dist-np.tril(dist)\n",
    "        aux = aux.reshape(size1**2,1)\n",
    "        sig = np.sqrt(0.5*np.mean(aux[np.where(aux>0)]))             \n",
    "    K = np.exp(-dist/sig**2);\n",
    "    return K, sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "bases = ['KPCA_SVM', 'KCCA_', 'KCCA_SVM', '_SVMrbf'] # Name of the baseline\n",
    "for base in bases:\n",
    "    # At his point we check whether the file where we want to store the results does or doesn't already exist.\n",
    "    # If it does we check if this baseline has been stored and, if so, we load it. If the baseline isn't in the file, we define it.\n",
    "    filename = 'Results/Baselines_'+database+'_'+str(folds)+'folds.pkl'\n",
    "    if os.path.exists(filename):\n",
    "        print (\"Loading existing model...\")\n",
    "        results = pickle.load( open( filename, \"rb\" ) )\n",
    "        if base in results:\n",
    "            print (\"... Model loaded\")\n",
    "        else:\n",
    "            results[base] = {}\n",
    "            results[base]['ACC'] = np.zeros((len(fold_tst),))\n",
    "            results[base]['AUC'] = np.zeros((len(fold_tst),))\n",
    "            results[base]['Kc'] = np.zeros((len(fold_tst),))\n",
    "            print (\"... Model defined\")\n",
    "    else:\n",
    "        results = {}\n",
    "        results[base] = {}\n",
    "        results[base]['ACC'] = np.zeros((len(fold_tst),))\n",
    "        results[base]['AUC'] = np.zeros((len(fold_tst),))\n",
    "        results[base]['Kc'] = np.zeros((len(fold_tst),))\n",
    "    \n",
    "    # We separate the baseline into the different options available\n",
    "    pipeline = base.split('_')\n",
    "    print('Training '+pipeline[0]+' FE and '+pipeline[1]+' Classifier')\n",
    "    for i in np.arange(len(fold_tst)):\n",
    "        \n",
    "        print('---------> Fold '+str(i)+' <---------')   \n",
    "        \n",
    "        if results[base]['ACC'][i] == 0.0:\n",
    "            # Splitting the data into training and test sets.\n",
    "            pos_tr = fold_tst[i][0]\n",
    "            pos_tst =  fold_tst[i][1]\n",
    "            Y_tr = Y[pos_tr] \n",
    "            Y_tst = Y[pos_tst]\n",
    "            encoder = LabelBinarizer()\n",
    "            Y_tr_b = encoder.fit_transform(Y_tr)\n",
    "            Y_tst_b = encoder.transform(Y_tst)\n",
    "            p_class = np.sum(Y_tst_b, axis=0)/np.sum(Y_tst_b)\n",
    "            X_tr = X[pos_tr,:]    \n",
    "            X_tst = X[pos_tst,:]\n",
    "            \n",
    "            # Normalizing the data\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X_tr)\n",
    "            X_tst = scaler.transform(X_tst)\n",
    "    \n",
    "            # Generating RBF kernel and calculating the gamma value.\n",
    "            K_tr, sig = rbf_kernel_sig(X_tr, X_tr)\n",
    "            K_tst, sig = rbf_kernel_sig(X_tst, X_tr, sig = sig)\n",
    "            \n",
    "            ##############################################\n",
    "            # Defining the feature extracting algorithm. #\n",
    "            ##############################################\n",
    "            print('Extracting features...')\n",
    "            if pipeline[0] == 'KPCA':\n",
    "                # KPCA\n",
    "                pca = PCA()\n",
    "                P_tr = pca.fit_transform(K_tr)\n",
    "                P_tst = pca.transform(K_tst)\n",
    "    \n",
    "                # Selecting the latent factors that explain 95% of the variance.\n",
    "                Kc = 0\n",
    "                while np.sum(pca.explained_variance_ratio_[:Kc]) < 0.95:\n",
    "                    Kc = Kc + 1 \n",
    "                results[base]['Kc'][i] = Kc\n",
    "                P_tr = P_tr[:,:Kc]\n",
    "                P_tst = P_tst[:,:Kc]      \n",
    "                print('... projections defined.')\n",
    "            elif pipeline[0] == 'KCCA':\n",
    "                # KCCA\n",
    "                cca = CCA(n_components = Y_tr_b.shape[1]-1).fit(K_tr, Y_tr_b)\n",
    "                results[base]['Kc'][i] = Y_tr_b.shape[1]-1\n",
    "                P_tr = cca.transform(K_tr)\n",
    "                P_tst = cca.transform(K_tst)\n",
    "                print('... projections defined.')\n",
    "            else:\n",
    "                # No feature extraction and, therefore, no kernel used.\n",
    "                P_tr = np.copy(X_tr)\n",
    "                P_tst = np.copy(X_tst)\n",
    "                print('... no projections defined.')\n",
    "            \n",
    "            ############################\n",
    "            # Training the classifier. #\n",
    "            ############################\n",
    "            print('Training the classifier...')\n",
    "            if pipeline[1] == 'SVM':\n",
    "                # SVM lineal\n",
    "                # Hyperparameters determined using grid search 10 fold cross validation.\n",
    "                grid = {\"C\": np.logspace(-4,4,11)}# l1 lasso l2 ridge\n",
    "                clf = SVC(kernel = 'linear')\n",
    "                clf_cv = GridSearchCV(clf, grid, cv=10)\n",
    "                clf_cv.fit(P_tr,Y_tr)\n",
    "                results[base]['ACC'][i] = clf_cv.score(P_tst,Y_tst)\n",
    "                results[base]['AUC'][i] = np.sum(calcAUC(clf_cv.decision_function(P_tst), Y_tst_b)*p_class)\n",
    "            elif pipeline[1] == 'SVMrbf':\n",
    "                # SVM rbf, no lineal.\n",
    "                # Hyperparameters determined using grid search 10 fold cross validation.\n",
    "                grid = {\"C\": np.logspace(-4,4,11), \"gamma\": np.array([0.125, 0.25, 0.5, 1, 2, 4, 8])/(np.sqrt(Y_tr_b.shape[1]))}\n",
    "                clf = SVC(kernel = 'rbf')\n",
    "                clf_cv = GridSearchCV(clf, grid, cv=10)\n",
    "                clf_cv.fit(P_tr,Y_tr)\n",
    "                results[base]['ACC'][i] = clf_cv.score(P_tst,Y_tst)  \n",
    "                results[base]['AUC'][i] = np.sum(calcAUC(clf_cv.decision_function(P_tst), Y_tst_b)*p_class)\n",
    "            elif pipeline[1] == 'GPLVM':\n",
    "                # AQUI VA EL GPLVM\n",
    "                print('CAMBIAME, POR FAVOR')\n",
    "            else:\n",
    "                try:\n",
    "                    results[base]['ACC'][i] = accuracy_score(Y_tst, np.argmax(cca.predict(K_tst),axis=1))\n",
    "                    results[base]['AUC'][i] = np.sum(calcAUC(cca.predict(K_tst), Y_tst_b)*p_class)\n",
    "                except:\n",
    "                    print('The selected classifier is not recognised.')\n",
    "            print('... classifier trained.\\n')\n",
    "            # Storing the results.\n",
    "            print(base + ' accuracy: %0.2f%%' %(results[base]['ACC'][i]*100))\n",
    "            print(base + ' AUC:      %0.3f' %(results[base]['AUC'][i]))\n",
    "            try:\n",
    "                with open(filename, 'wb') as output:\n",
    "                    pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
    "            except:\n",
    "                time.sleep(100)\n",
    "        else:\n",
    "            print('Fold previously trained. ' + base + ' accuracy: %0.2f%%\\n                                       AUC: %0.2f' %(results[base]['ACC'][i]*100, results[base]['AUC'][i]))\n",
    "    \n",
    "    print(base +' mean accuracy: %0.2f +/- %0.2f%%' %(np.mean(results[base]['ACC']*100) , np.std(results[base]['ACC']*100)))\n",
    "    print(base +' mean AUC:      %0.2f +/- %0.3f' %(np.mean(results[base]['AUC']*100) , np.std(results[base]['AUC']*100)))\n",
    "    print(base +' mean Kc:       %0.2f +/- %0.2f' %(np.mean(results[base]['Kc']) , np.std(results[base]['Kc'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
